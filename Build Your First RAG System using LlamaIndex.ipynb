{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACfLrpwlo8Nl"
   },
   "source": [
    "# Build Your First RAG System\n",
    "\n",
    "1. Data Ingestion.\n",
    "2. Indexing.\n",
    "3. Retriever.\n",
    "4. Response Synthesizer.\n",
    "5. Querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the required packages by executing the below commands in either Anaconda Prompt (in Windows) or Terminal (in Linux or Mac OS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9291,
     "status": "ok",
     "timestamp": 1703361268107,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "Elvu26cIedWC",
    "outputId": "529ae34f-17a9-4724-b391-f1803279dfed"
   },
   "source": [
    "pip install llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommonded to store the API keys in a '.env' file, separate from the code.\n",
    "Plesae follow the below steps.\n",
    "1. Create a text file with the name '.env'\n",
    "2. Enter your api key in this format OPENAI_API_KEY='sk-e8943u9ru4982............'\n",
    "3. Save and close the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, as shown below you can provide the path of the '.env' file to 'load_dotenv' method.\n",
    "This will load any API keys stored in the '.env' file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cKlax-updNW-"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load_dotenv('/home/santhosh/Projects/courses/Pinnacle/.env')\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This setup ensures that our API key remains secure and easily configurable. Always remember to keep your `.env` file secure and avoid including it in version control.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLtBXZ0xDtmQ"
   },
   "source": [
    "# Stage 1: Data Ingestion\n",
    "\n",
    "## Data Loaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the data from a PDF file. For this, we will use the SimpleDirectoryReader class from LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index\n",
      "  Downloading llama_index-0.12.44-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting llama-index-agent-openai<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl.metadata (439 bytes)\n",
      "Collecting llama-index-cli<0.5,>=0.4.2 (from llama-index)\n",
      "  Downloading llama_index_cli-0.4.3-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting llama-index-core<0.13,>=0.12.44 (from llama-index)\n",
      "  Downloading llama_index_core-0.12.44-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-embeddings-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
      "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting llama-index-llms-openai<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.6,>=0.5.0 (from llama-index)\n",
      "  Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl.metadata (440 bytes)\n",
      "Collecting llama-index-program-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_program_openai-0.3.2-py3-none-any.whl.metadata (473 bytes)\n",
      "Collecting llama-index-question-gen-openai<0.4,>=0.3.0 (from llama-index)\n",
      "  Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl.metadata (492 bytes)\n",
      "Collecting llama-index-readers-file<0.5,>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_file-0.4.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
      "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: nltk>3.8.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index) (3.9.1)\n",
      "Requirement already satisfied: openai>=1.14.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.63.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (3.11.12)\n",
      "Collecting aiosqlite (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.0.0 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading banks-2.1.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.6.7)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2025.5.1)\n",
      "Requirement already satisfied: httpx in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.28.0)\n",
      "Collecting llama-index-workflows<2,>=1.0.1 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading llama_index_workflows-1.0.1-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (3.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.10.6)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (2.32.3)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.44->llama-index) (2.0.38)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-core<0.13,>=0.12.44->llama-index) (0.9.0)\n",
      "Collecting wrapt (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13,>=0.12.44->llama-index) (1.18.3)\n",
      "Collecting griffe (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading griffe-1.7.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.1.4)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\sivak\\appdata\\roaming\\python\\python313\\site-packages (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (4.3.6)\n",
      "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5,>=0.4.0->llama-index)\n",
      "  Downloading openai-1.92.2-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (4.12.3)\n",
      "Requirement already satisfied: pandas<2.3.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.2.3)\n",
      "Requirement already satisfied: pypdf<6,>=5.1.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-index-readers-file<0.5,>=0.4.0->llama-index) (5.3.0)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5,>=0.4.0->llama-index)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl.metadata (252 bytes)\n",
      "Collecting pydantic>=2.8.0 (from llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (0.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai>=1.14.0->llama-index-agent-openai<0.5,>=0.4.0->llama-index) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\sivak\\appdata\\roaming\\python\\python313\\site-packages (from httpx->llama-index-core<0.13,>=0.12.44->llama-index) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx->llama-index-core<0.13,>=0.12.44->llama-index) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core<0.13,>=0.12.44->llama-index) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (2025.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.44->llama-index) (0.7.0)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic>=2.8.0->llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core<0.13,>=0.12.44->llama-index) (0.4.6)\n",
      "Collecting llama-cloud==0.1.26 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud-0.1.26-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.37-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.37 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.37-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cloud-services>=0.6.37->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (8.1.8)\n",
      "INFO: pip is looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.36-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.36 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.36-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.35-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting llama-cloud-services>=0.6.35 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.35-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_parse-0.6.34-py3-none-any.whl.metadata (6.9 kB)\n",
      "INFO: pip is still looking at multiple versions of llama-cloud-services to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting llama-cloud-services>=0.6.32 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
      "  Downloading llama_cloud_services-0.6.34-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting platformdirs (from banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index)\n",
      "  Downloading platformdirs-4.3.8-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from llama-cloud-services>=0.6.32->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index) (1.0.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file<0.5,>=0.4.0->llama-index) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.31.0->llama-index-core<0.13,>=0.12.44->llama-index) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13,>=0.12.44->llama-index) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13,>=0.12.44->llama-index) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from dataclasses-json->llama-index-core<0.13,>=0.12.44->llama-index) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\sivak\\appdata\\roaming\\python\\python313\\site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13,>=0.12.44->llama-index) (24.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sivak\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13,>=0.12.44->llama-index) (3.0.2)\n",
      "Downloading llama_index-0.12.44-py3-none-any.whl (7.1 kB)\n",
      "Downloading llama_index_agent_openai-0.4.11-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_cli-0.4.3-py3-none-any.whl (28 kB)\n",
      "Downloading llama_index_core-0.12.44-py3-none-any.whl (7.6 MB)\n",
      "   ---------------------------------------- 0.0/7.6 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 3.9/7.6 MB 20.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.6/7.6 MB 20.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.6/7.6 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading banks-2.1.3-py3-none-any.whl (28 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading llama_index_llms_openai-0.4.7-py3-none-any.whl (25 kB)\n",
      "Downloading llama_index_multi_modal_llms_openai-0.5.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading llama_index_program_openai-0.3.2-py3-none-any.whl (6.1 kB)\n",
      "Downloading llama_index_question_gen_openai-0.3.1-py3-none-any.whl (3.7 kB)\n",
      "Downloading llama_index_readers_file-0.4.9-py3-none-any.whl (40 kB)\n",
      "Downloading llama_index_workflows-1.0.1-py3-none-any.whl (36 kB)\n",
      "Downloading openai-1.92.2-py3-none-any.whl (753 kB)\n",
      "   ---------------------------------------- 0.0/753.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 753.3/753.3 kB 14.7 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Downloading llama_index_indices_managed_llama_cloud-0.7.7-py3-none-any.whl (16 kB)\n",
      "Downloading llama_cloud-0.1.26-py3-none-any.whl (266 kB)\n",
      "Downloading llama_index_instrumentation-0.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
      "Downloading llama_parse-0.6.34-py3-none-any.whl (4.9 kB)\n",
      "Downloading llama_cloud_services-0.6.34-py3-none-any.whl (39 kB)\n",
      "Downloading platformdirs-4.3.8-py3-none-any.whl (18 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.7.3-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: striprtf, filetype, dirtyjson, wrapt, typing-inspection, setuptools, pydantic-core, platformdirs, griffe, aiosqlite, pydantic, deprecated, openai, llama-index-instrumentation, llama-cloud, banks, llama-index-workflows, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
      "\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   - --------------------------------------  1/31 [filetype]\n",
      "   -- -------------------------------------  2/31 [dirtyjson]\n",
      "   -- -------------------------------------  2/31 [dirtyjson]\n",
      "   -- -------------------------------------  2/31 [dirtyjson]\n",
      "   --- ------------------------------------  3/31 [wrapt]\n",
      "   --- ------------------------------------  3/31 [wrapt]\n",
      "  Attempting uninstall: setuptools\n",
      "   --- ------------------------------------  3/31 [wrapt]\n",
      "    Found existing installation: setuptools 75.6.0\n",
      "   --- ------------------------------------  3/31 [wrapt]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "    Uninstalling setuptools-75.6.0:\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "      Successfully uninstalled setuptools-75.6.0\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "  Attempting uninstall: pydantic-core\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "    Found existing installation: pydantic_core 2.27.2\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "    Uninstalling pydantic_core-2.27.2:\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "      Successfully uninstalled pydantic_core-2.27.2\n",
      "   ------ ---------------------------------  5/31 [setuptools]\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "  Attempting uninstall: platformdirs\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "    Found existing installation: platformdirs 4.3.6\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "    Uninstalling platformdirs-4.3.6:\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "      Successfully uninstalled platformdirs-4.3.6\n",
      "   ------- --------------------------------  6/31 [pydantic-core]\n",
      "   --------- ------------------------------  7/31 [platformdirs]\n",
      "   --------- ------------------------------  7/31 [platformdirs]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ---------- -----------------------------  8/31 [griffe]\n",
      "   ----------- ----------------------------  9/31 [aiosqlite]\n",
      "   ----------- ----------------------------  9/31 [aiosqlite]\n",
      "  Attempting uninstall: pydantic\n",
      "   ----------- ----------------------------  9/31 [aiosqlite]\n",
      "    Found existing installation: pydantic 2.10.6\n",
      "   ----------- ----------------------------  9/31 [aiosqlite]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "    Uninstalling pydantic-2.10.6:\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "      Successfully uninstalled pydantic-2.10.6\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   ------------ --------------------------- 10/31 [pydantic]\n",
      "   -------------- ------------------------- 11/31 [deprecated]\n",
      "  Attempting uninstall: openai\n",
      "   -------------- ------------------------- 11/31 [deprecated]\n",
      "    Found existing installation: openai 1.63.2\n",
      "   -------------- ------------------------- 11/31 [deprecated]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "    Uninstalling openai-1.63.2:\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "      Successfully uninstalled openai-1.63.2\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   --------------- ------------------------ 12/31 [openai]\n",
      "   ---------------- ----------------------- 13/31 [llama-index-instrumentation]\n",
      "   ---------------- ----------------------- 13/31 [llama-index-instrumentation]\n",
      "   ---------------- ----------------------- 13/31 [llama-index-instrumentation]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------ --------------------- 14/31 [llama-cloud]\n",
      "   ------------------- -------------------- 15/31 [banks]\n",
      "   ------------------- -------------------- 15/31 [banks]\n",
      "   ------------------- -------------------- 15/31 [banks]\n",
      "   -------------------- ------------------- 16/31 [llama-index-workflows]\n",
      "   -------------------- ------------------- 16/31 [llama-index-workflows]\n",
      "   -------------------- ------------------- 16/31 [llama-index-workflows]\n",
      "   -------------------- ------------------- 16/31 [llama-index-workflows]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   --------------------- ------------------ 17/31 [llama-index-core]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ----------------------- ---------------- 18/31 [llama-index-readers-file]\n",
      "   ------------------------ --------------- 19/31 [llama-index-llms-openai]\n",
      "   ------------------ --------- 20/31 [llama-index-indices-managed-llama-cloud]\n",
      "   ------------------------- ------------ 21/31 [llama-index-embeddings-openai]\n",
      "   ---------------------------- ----------- 22/31 [llama-cloud-services]\n",
      "   ---------------------------- ----------- 22/31 [llama-cloud-services]\n",
      "   ---------------------------- ----------- 22/31 [llama-cloud-services]\n",
      "   ---------------------------- ----------- 22/31 [llama-cloud-services]\n",
      "   ----------------------------- ---------- 23/31 [llama-parse]\n",
      "   ------------------------ ------- 24/31 [llama-index-multi-modal-llms-openai]\n",
      "   -------------------------------- ------- 25/31 [llama-index-cli]\n",
      "   -------------------------------- ------- 25/31 [llama-index-cli]\n",
      "   -------------------------------- ------- 25/31 [llama-index-cli]\n",
      "   --------------------------------- ------ 26/31 [llama-index-agent-openai]\n",
      "   ------------------------------------ --- 28/31 [llama-index-program-openai]\n",
      "   -------------------------------------- - 30/31 [llama-index]\n",
      "   ---------------------------------------- 31/31 [llama-index]\n",
      "\n",
      "Successfully installed aiosqlite-0.21.0 banks-2.1.3 deprecated-1.2.18 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.3 llama-cloud-0.1.26 llama-cloud-services-0.6.34 llama-index-0.12.44 llama-index-agent-openai-0.4.11 llama-index-cli-0.4.3 llama-index-core-0.12.44 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.7.7 llama-index-instrumentation-0.2.0 llama-index-llms-openai-0.4.7 llama-index-multi-modal-llms-openai-0.5.1 llama-index-program-openai-0.3.2 llama-index-question-gen-openai-0.3.1 llama-index-readers-file-0.4.9 llama-index-readers-llama-parse-0.4.0 llama-index-workflows-1.0.1 llama-parse-0.6.34 openai-1.92.2 platformdirs-4.3.8 pydantic-2.11.7 pydantic-core-2.33.2 setuptools-80.9.0 striprtf-0.0.26 typing-inspection-0.4.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gGfPPk4gBAkQ"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(input_files=['files/us_constitution.pdf']).load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then check the type of the `documents` variable and the total number of pages read from the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the datatype and length of the loaded documents\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of pages read from the PDF\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To understand the structure of the loaded documents, let's retrieve the first document, which corresponds to the first page of the PDF:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id_='cb8d2111-ccd7-42e4-adb2-0d62bbd434be', embedding=None, metadata={'page_label': '1', 'file_name': 'us_constitution.pdf', 'file_path': 'files\\\\us_constitution.pdf', 'file_type': 'application/pdf', 'file_size': 170876, 'creation_date': '2025-06-07', 'last_modified_date': '2025-06-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='The United States Constitution \\n W e the People of the United States, in Order to form a more perfect \\n Union, establish Justice, insure domestic T ranquility , provide for the \\n common defence, promote the general W elfare, and secure the \\n Blessings of Liberty to ourselves and our Posterity , do ordain and \\n establish this Constitution for the United States of America. \\nThe Constitutional Con v ention \\n Article I \\n Section 1: Congress \\n All legislative Powers herein granted shall be vested in a Congress of \\n the United States, which shall consist of a Senate and House of \\n Representatives. \\nSection 2: The House of Representatives ', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the first document (essentially the first page in the PDF)\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access specific attributes of the document, such as its ID and metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cb8d2111-ccd7-42e4-adb2-0d62bbd434be'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the ID of the first document\n",
    "documents[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cb8d2111-ccd7-42e4-adb2-0d62bbd434be'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'us_constitution.pdf',\n",
       " 'file_path': 'files\\\\us_constitution.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 170876,\n",
       " 'creation_date': '2025-06-07',\n",
       " 'last_modified_date': '2025-06-07'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the metadata of the first document\n",
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States Constitution \n",
      " W e the People of the United States, in Order to form a more perfect \n",
      " Union, establish Justice, insure domestic T ranquility , provide for the \n",
      " common defence, promote the general W elfare, and secure the \n",
      " Blessings of Liberty to ourselves and our Posterity , do ordain and \n",
      " establish this Constitution for the United States of America. \n",
      "The Constitutional Con v ention \n",
      " Article I \n",
      " Section 1: Congress \n",
      " All legislative Powers herein granted shall be vested in a Congress of \n",
      " the United States, which shall consist of a Senate and House of \n",
      " Representatives. \n",
      "Section 2: The House of Representatives \n"
     ]
    }
   ],
   "source": [
    "# Get the text content of the first document\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv9VQB-EdsEd"
   },
   "source": [
    "## Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to prepare our document for embedding and interaction with a large language model. We will use the OpenAI API for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "RTOBfe1hc2zu"
   },
   "outputs": [],
   "source": [
    "# Embedding Model\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UD_RkiXf7Cm"
   },
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, let's set up our large language model (LLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "6q6O3wusigcW"
   },
   "outputs": [],
   "source": [
    "# LLM\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the large language model\n",
    "llm = OpenAI(model= \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vseCdqiFj7W0"
   },
   "source": [
    "# Stage 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "T9NxcrBpeprP"
   },
   "outputs": [],
   "source": [
    "# Indexing\n",
    "from llama_index.core import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the `VectorStoreIndex` class to create an index from the loaded documents. We pass the document chunks, embedding model, and LLM to the `from_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an index from the documents using the embedding model and LLM\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA9czIv0sqe_"
   },
   "source": [
    "# Stage 3: Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set up a retriever to query our indexed documents. This allows us to retrieve relevant information based on our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "8-E66LtRjgT4"
   },
   "outputs": [],
   "source": [
    "# Setting up the Index as Retriever\n",
    "retriever = index.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `as_retriever` method converts our index into a retriever, and the `retrieve` method allows us to query the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "foOrz7q-oAJl"
   },
   "outputs": [],
   "source": [
    "# Retrieve information based on the query \"What are Transformers?\"\n",
    "retrieved_nodes = retriever.retrieve(\"What is US consulate?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the metadata of the retrieved nodes to understand the source of the information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata provides details such as the page label, file name, file path, file type, and other relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '1',\n",
       " 'file_name': 'us_constitution.pdf',\n",
       " 'file_path': 'files\\\\us_constitution.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 170876,\n",
       " 'creation_date': '2025-06-07',\n",
       " 'last_modified_date': '2025-06-07'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the metadata of the first retrieved node\n",
    "retrieved_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's access the ID of the first retrieved node, which is a unique identifier for the first node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7c7cfe83-7aa6-4679-a020-fcfb176cb0dc'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID of the first retrieved node\n",
    "retrieved_nodes[0].id_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can access the node_id attribute, which typically holds the same value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7c7cfe83-7aa6-4679-a020-fcfb176cb0dc'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the node_id of the first retrieved node\n",
    "retrieved_nodes[0].node_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's explore the `node` attribute of the retrieved node. This attribute contains a `TextNode` object, which holds all the relevant information extracted during the retrieval process: The `TextNode` object includes various details such as metadata and text content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1703361342847,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "EXpkVs2RoHsA",
    "outputId": "d3f9b5b0-d90c-43b6-b194-8d05c249aa97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextNode(id_='7c7cfe83-7aa6-4679-a020-fcfb176cb0dc', embedding=None, metadata={'page_label': '1', 'file_name': 'us_constitution.pdf', 'file_path': 'files\\\\us_constitution.pdf', 'file_type': 'application/pdf', 'file_size': 170876, 'creation_date': '2025-06-07', 'last_modified_date': '2025-06-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='cb8d2111-ccd7-42e4-adb2-0d62bbd434be', node_type='4', metadata={'page_label': '1', 'file_name': 'us_constitution.pdf', 'file_path': 'files\\\\us_constitution.pdf', 'file_type': 'application/pdf', 'file_size': 170876, 'creation_date': '2025-06-07', 'last_modified_date': '2025-06-07'}, hash='957cfd697d9ed8ed93e148b4984bd8058ff7be341dfd2b196fe1d72017d2cc69')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The United States Constitution \\n W e the People of the United States, in Order to form a more perfect \\n Union, establish Justice, insure domestic T ranquility , provide for the \\n common defence, promote the general W elfare, and secure the \\n Blessings of Liberty to ourselves and our Posterity , do ordain and \\n establish this Constitution for the United States of America. \\nThe Constitutional Con v ention \\n Article I \\n Section 1: Congress \\n All legislative Powers herein granted shall be vested in a Congress of \\n the United States, which shall consist of a Senate and House of \\n Representatives. \\nSection 2: The House of Representatives', mimetype='text/plain', start_char_idx=0, end_char_idx=639, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the full node object of the first retrieved node\n",
    "retrieved_nodes[0].node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract and inspect the text content of this node to understand the retrieved information better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703361344340,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "c5KctGWPLi7u",
    "outputId": "e463cbb7-d169-456c-bccb-ab15004f711a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States Constitution \n",
      " W e the People of the United States, in Order to form a more perfect \n",
      " Union, establish Justice, insure domestic T ranquility , provide for the \n",
      " common defence, promote the general W elfare, and secure the \n",
      " Blessings of Liberty to ourselves and our Posterity , do ordain and \n",
      " establish this Constitution for the United States of America. \n",
      "The Constitutional Con v ention \n",
      " Article I \n",
      " Section 1: Congress \n",
      " All legislative Powers herein granted shall be vested in a Congress of \n",
      " the United States, which shall consist of a Senate and House of \n",
      " Representatives. \n",
      "Section 2: The House of Representatives\n"
     ]
    }
   ],
   "source": [
    "# Access the text content of the first retrieved node\n",
    "print(retrieved_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '16',\n",
       " 'file_name': 'us_constitution.pdf',\n",
       " 'file_path': 'files\\\\us_constitution.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 170876,\n",
       " 'creation_date': '2025-06-07',\n",
       " 'last_modified_date': '2025-06-07'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He shall have Power , by and with the Advice and Consent of the \n",
      " Senate, to make T reaties, provided two thirds of the Senators present \n",
      " concur; and he shall nominate, and by and with the Advice and \n",
      " Consent of the Senate, shall appoint Ambassadors, other public \n",
      " Ministers and Consuls, Judges of the supreme Court, and all other \n",
      " Of ficers of the United States, whose Appointments are not herein \n",
      " otherwise provided for , and which shall be established by Law: but the \n",
      " Congress may by Law vest the Appointment of such inferior Of ficers, \n",
      " as they think proper , in the President alone, in the Courts of Law , or in \n",
      " the Heads of Departments. \n",
      " The President shall have Power to fill up all V acancies that may happen \n",
      " during the Recess of the Senate, by granting Commissions which shall \n",
      " expire at the End of their next Session. \n",
      "Section 3 \n",
      " He shall from time to time give to the Congress Information of the State \n",
      " of the Union, and recommend to their Consideration such Measures as \n",
      " he shall judge necessary and expedient; he may , on extraordinary \n",
      " Occasions, convene both Houses, or either of them, and in Case of \n",
      " Disagreement between them, with Respect to the T ime of Adjournment, \n",
      " he may adjourn them to such T ime as he shall think proper; he shall \n",
      " receive Ambassadors and other public Ministers; he shall take Care\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_nodes[1].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ty19sHbWxoEu"
   },
   "source": [
    "# Stage 4: Response Synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to synthesize responses from our large language model (LLM). For this, we use the `get_response_synthesizer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "TnLdxijaxw80"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the `get_response_synthesizer` function takes our LLM as an argument and returns a synthesizer object that will help generate coherent responses to our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the response synthesizer with the LLM\n",
    "response_synthesizer = get_response_synthesizer(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orz-nHJYyz0u"
   },
   "source": [
    "## Stage 5: Query Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up a query engine. This engine will allow us to query our indexed documents and receive synthesized responses from the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "EiHo7R3K0OH3"
   },
   "outputs": [],
   "source": [
    "# Create a query engine using the index, LLM, and response synthesizer\n",
    "query_engine = index.as_query_engine(llm=llm, response_synthesizer=response_synthesizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `as_query_engine` method from our index object to create a query engine, passing the LLM and response synthesizer as arguments.\n",
    "\n",
    "With our query engine ready, we can now query the LLM using natural language:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "dTCGOKvI1Zj_"
   },
   "outputs": [],
   "source": [
    "# Query the LLM using the query engine\n",
    "response = query_engine.query(\"What is us consulate?\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this command, we query the LLM with the question \"What are Transformers?\" and store the response in the `response` variable.\n",
    "\n",
    "To view the response generated by the LLM, we can access the `response` attribute:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1703361376718,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "mTgwMcaJ1nhT",
    "outputId": "e571e28d-711d-4c45-e7f7-0fd882677ee6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A U.S. consulate is a diplomatic mission that represents the United States in a foreign country. It is responsible for assisting U.S. citizens abroad, promoting American interests, and facilitating trade and communication between the U.S. and the host country. Consulates handle various functions, including issuing visas, providing support to American citizens, and fostering economic and cultural relations.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the response from the LLM\n",
    "response.response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the synthesized answer to our query.\n",
    "\n",
    "We can further analyze the response by checking its length and inspecting the source nodes used to generate it:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These commands provide the length of the response and the number of source nodes, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "409"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the response\n",
    "len(response.response) # number of characters in the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1703361403705,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "aRMcVB1nQBbp",
    "outputId": "f84a6390-2f65-4ada-d322-5a63097a5187"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of source nodes\n",
    "len(response.source_nodes)  # list of 2 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3c11e22d-2950-4697-aee3-a6dc137fed3d'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID and metadata of the first source node\n",
    "response.source_nodes[0].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '4',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data/transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-06-11',\n",
       " 'last_modified_date': '2024-03-27'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the ID and metadata of the second source node\n",
    "response.source_nodes[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c32c5c7e-fd92-4147-8e3d-79817f93a273'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[1].id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'page_label': '6',\n",
       " 'file_name': 'transformers.pdf',\n",
       " 'file_path': 'data/transformers.pdf',\n",
       " 'file_type': 'application/pdf',\n",
       " 'file_size': 2215244,\n",
       " 'creation_date': '2024-06-11',\n",
       " 'last_modified_date': '2024-03-27'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes[1].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRUMuoRK7qvt"
   },
   "source": [
    "# End to End RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this final section, we will integrate everything we have learned to create a complete end-to-end Retrieval-Augmented Generation (RAG) pipeline. This pipeline will read documents, index them, and allow us to query the indexed data using a large language model (LLM).\n",
    "\n",
    "Let's walk through the entire process step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we import the necessary libraries and load our documents from a specified directory. We use the `SimpleDirectoryReader` class from LlamaIndex to read all documents in the 'data' directory:\n",
    "\n",
    "\n",
    "- The `SimpleDirectoryReader` reads the documents in the 'data' directory and stores them in the `documents` variable.\n",
    "\n",
    "- Next, we initialize our large language model (LLM) and embedding model. For this demonstration, we assume that these models have already been initialized and are available as `llm` and `embed_model`:\n",
    "\n",
    "- With our documents and models ready, we proceed to create an index. This index will facilitate efficient retrieval of information from our documents. Here, we use the `VectorStoreIndex` class to create an index from the loaded documents, embedding model, and LLM.\n",
    "\n",
    "- We then set up a query engine that will allow us to query the indexed documents using natural language. The query engine is created from our index and LLM:\n",
    "\n",
    "- Finally, we use the query engine to ask a question and receive a response from the LLM. In this example, we query the different types of Transformer models:\n",
    "\n",
    "- The `query` method sends the question to the LLM, which retrieves relevant information from the indexed documents and synthesizes a response. The response is then printed to the console.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1806,
     "status": "ok",
     "timestamp": 1703361456294,
     "user": {
      "displayName": "Ravi Theja",
      "userId": "12148656718425770960"
     },
     "user_tz": -330
    },
    "id": "HhAb3o0l7wwD",
    "outputId": "e683a5ab-23ac-486f-ba11-0d4b51bd8499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer models mentioned include the base model and the big model. The base model achieves a BLEU score of 27.3 for English-to-German translation and 38.1 for English-to-French translation, while the big model achieves higher scores of 28.4 and 41.0, respectively. These models differ in their configurations and training costs, with the big model outperforming the base model and other previously published models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "\n",
    "# Load data from the specified directory\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# Initialize LLM and embedding model (assumed to be pre-initialized)\n",
    "llm = llm\n",
    "embed_model = embed_model\n",
    "\n",
    "# Create an index from the documents using the embedding model and LLM\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model, llm=llm)\n",
    "\n",
    "# Create a query engine from the index and LLM\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Query the LLM and print the response\n",
    "print(query_engine.query(\"What are the different types of Transformer Models?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional encodings are needed in transformers because the model itself doesn't have any inherent sense of position or order of the sequence elements. Unlike recurrent neural networks, transformers process input data in parallel rather than sequentially, which makes them more efficient but also means they don't inherently understand the order of the data. Positional encodings are used to give the model some information about the relative positions of the elements in the sequence.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"Why do we need positional encodings in transformer?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoder and decoder are key components of the Transformer model architecture. The encoder is made up of a stack of six identical layers, each with two sub-layers. The first sub-layer is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. Residual connections are employed around each of the two sub-layers, followed by layer normalization. \n",
      "\n",
      "The decoder, like the encoder, is composed of a stack of six identical layers. However, in addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Residual connections are also used around each of the sub-layers in the decoder, followed by layer normalization. The self-attention sub-layer in the decoder stack is modified to prevent positions from attending to subsequent positions, ensuring that the predictions for a given position can depend only on the known outputs at positions less than that position.\n"
     ]
    }
   ],
   "source": [
    "print(query_engine.query(\"What are Encoder and Decoder blocks in transformer?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer architecture you should choose for generating document embeddings is the Encoder part of the Transformer model. The encoder maps an input sequence of symbol representations to a sequence of continuous representations, which can be used as document embeddings. It is composed of a stack of identical layers, each with two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.\n"
     ]
    }
   ],
   "source": [
    "query = \"If I want to generate document embeddings, then which type of Transformer Architecture I must choose?\"\n",
    "print(query_engine.query(query).response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate document embeddings, you should choose the Encoder part of the Transformer Architecture. The Encoder maps an input sequence of symbol representations to a sequence of continuous representations, which can be used as document embeddings.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I want to generate document embeddings, \n",
    "then which type of Transformer Architecture I must choose among Encoders, Decoders or Encoder-Decorder?\"\"\"\n",
    "\n",
    "print(query_engine.query(query).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By following these steps, we have created a fully functional end-to-end RAG pipeline. This pipeline can ingest documents, index them, and answer natural language queries using a powerful combination of LlamaIndex and OpenAI's models. This demonstrates the practical application of RAG systems in extracting and synthesizing information from large datasets.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
